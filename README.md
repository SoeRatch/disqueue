# Disqueue

**Disqueue** is a lightweight distributed job queue system inspired by Celery and BullMQ. Built with FastAPI, Redis Streams, and Docker, it supports job prioritization, retries, cancellation, dead-letter queue and Redis-powered idempotency and deduplication â€” all while remaining modular, extensible, and developer-friendly.

---
## Table of Contents

- [Features](#features)  
- [Stack](#stack)
- [Architecture Overview](#architecture-overview)
- [Components](#components) 
- [Directory Structure](#directory-structure)  
- [Getting Started](#getting-started)  
  - [Prerequisites](#prerequisites)  
  - [Setup Instructions](#setup-instructions)  
- [Usage](#example-usage)  
  - [Queue a Job](#1-queue-a-job)  
  - [Check Job Status](#2-check-job-status)  
  - [Simulate a Failing Job](#3-simulate-a-failing-job)  
- [Retry Mechanism](#retry-mechanism)  
- [Dead-letter Queue (DLQ)](#dead-letter-queue-dlq)
- [Idempotency & Deduplication](#idempotency--deduplication)
- [Configuration](#configuration)  
- [Whatâ€™s Next](#whats-next) 
- [Technologies Used](#technologies-used)  
- [Author](#author)  
- [License](#license)

---

## Features

- **Multiple Queues** â€“ Register and manage multiple job queues declaratively.
- **Priority Handling** â€“ Supports `high`, `medium`, `low` and `default` priority job queues.
- **Retry Mechanism** â€“ Automatic retries with configurable strategy and limits.
- **Dead-letter Queue (DLQ)** â€“ Failed jobs are automatically moved to a DLQ after exceeding retry limit for inspection or manual retry.
- **Job Cancellation** â€“ Cancel jobs before they are processed by a worker.
- **Idempotency & Deduplication** â€“ Redis-powered lock mechanism ensures a job is never processed by more than one worker simultaneously.
- **Graceful Shutdown** â€“ Worker completes the current job cleanly on SIGINT/SIGTERM.
- **Redis Integration** â€“ Uses Redis Streams and Hashes for job management.
- **Dockerized** â€“ Easily reproducible local development environment.
- **FastAPI API Layer** â€“ REST interface for job submission, status, cancellation, and queue discovery.
- **Modular Design** â€“ Decoupled architecture for clean separation of concerns.
- **Easily Extensible** â€“ Designed with modularity in mind to support open-source growth.

---

## Stack

- **FastAPI** â€“ REST API framework.
- **Redis Streams** â€“ Message broker.
- **Python** â€“ Worker logic and APIs.
- **Docker & Docker Compose** â€“ Containerization.
- **Pydantic** â€“ Typed settings and schema validation.


---

## Architecture Overview

- Jobs are added to Redis Streams based on queue name and priority.
- Job metadata (status, retry count, last stream ID, etc.) is tracked in Redis Hashes.
- Workers poll queues using a configurable priority order.
- Deduplication ensures only one worker processes a job at a time.
- Failed jobs are retried up to a max retry limit, and moved to DLQ after retries exceed limit.
- Each worker is aware of multiple queues and priorities using a central registry.
- Cancelled jobs are acknowledged to maintain stream offsets and avoid reprocessing.
- Worker logic is modularized via a StreamManager (stream polling) and JobProcessor (execution, retries, DLQ, deduplication).
- Workers support **graceful shutdown**, completing the in-progress job before exiting.

---

## Components

### `api/` â€“ FastAPI Service
- POST `/jobs/` â€“ Submit jobs with payload, priority, and queue.
- GET `/jobs/{job_id}` â€“ Check status of a specific job.
- POST `/jobs/{job_id}/cancel` â€“ Cancel a job if it's still queued or retrying.
- GET `/queues/` â€“ List registered queues and configurations.

### `core/worker.py` â€“ Main Worker Loop
- Loads all registered queues and initializes `QueueStreamManager`.
- Delegates job execution to `JobProcessor`.
- Supports safe exit on shutdown signal.

### `core/queue_config.py` â€“ Queue Registration
- Declarative queue registration via config.
- Central registry supports multiple named queues with custom priority schemes.

### `core/stream_manager.py` â€“ QueueStreamManager
- Reads from Redis streams using `XREAD`.
- Manages stream offset tracking (`last_id`).
- Polls in priority order.

### `core/processor.py` â€“ JobProcessor
- Core job logic:
  - Deduplication
  - Status updates
  - Retry handling
  - DLQ fallback

### `infrastructure/redis_job_store.py`
- Redis interface for enqueueing, job status, metadata, and stream tracking.
- Used by `JobProcessor` and `QueueStreamManager`.

### `utils/deduplication.py`
- Provides a `@deduplicated()` decorator using Redis `SET NX` locks.
- Ensures only the first worker to acquire the lock processes the job.
- Automatically releases the lock on failure or marks it `done` on success.

---

## Directory Structure

```
disqueue/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py               # FastAPI entry point
â”‚   â”œâ”€â”€ models.py             # Request/response schemas
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ job_routes.py     # Job-related API endpoints
â”‚       â””â”€â”€ queue_routes.py   # Queue-related API endpoints
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ logging_config.py     # Sets up logging format and levels
â”‚   â”œâ”€â”€ queue_registry.py     # Declares and registers supported queues and priorities
â”‚   â””â”€â”€ settings.py           # Loads env vars and app settings via Pydantic
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ processor.py          # Core job logic: retry, DLQ, status, deduplication
â”‚   â”œâ”€â”€ queue_config.py       # Models for queue configs used by registry
â”‚   â”œâ”€â”€ registry.py           # Central place for accessing registered queues
â”‚   â”œâ”€â”€ status.py             # Status enum and helpers
â”‚   â”œâ”€â”€ stream_manager.py     # Polls Redis Streams in priority order
â”‚   â””â”€â”€ worker.py             # Main worker loop and graceful shutdown logic
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ redis_conn.py         # Sets up Redis connection
â”‚   â””â”€â”€ redis_job_store.py    # Abstractions for enqueuing, tracking, and DLQ
â”œâ”€â”€ retry/
â”‚   â”œâ”€â”€ factory.py            # Returns retry strategy instance based on config
â”‚   â””â”€â”€ strategies.py         # Fixed and exponential retry implementations
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ deduplication.py      # Redis lock decorator to prevent duplicate execution
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile.api
â”œâ”€â”€ Dockerfile.worker
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## Getting Started

### Prerequisites

- [Docker](https://www.docker.com/get-started)
- [Docker Compose](https://docs.docker.com/compose/install/)

### Setup Instructions

1. **Clone the Repository**:

    ```bash
    git clone https://github.com/SoeRatch/disqueue.git
    cd disqueue
    ```

2. **Create the `.env` File**:

   Copy the provided `.env.example` file and update it with your local environment credentials:

   ```bash
   cp .env.example .env
   ```

   > **Note**: Never commit the `.env` file to version control. It should be ignored in `.gitignore`.


3. **Start Docker Services**:

    ```bash
    docker compose up --build -d
    ```

    On subsequent runs:
    ```bash
    docker compose up -d
    ```

    Services started:
    - `api` at [http://localhost:8000](http://localhost:8000)
    - `worker` (background processor)
    - `redis` (stream/message broker)
    
    Visit the API: [http://localhost:8000/docs](http://localhost:8000/docs)
  
4. **(Optional) Extend Worker Shutdown Timeout**:
    To prevent Docker from force-killing the worker while itâ€™s processing a job, you can extend the shutdown grace period in `docker-compose.yml`:

    ```yaml
    services:
      worker:
        stop_grace_period: 90s
    ```

---

## Example Usage

### 1. Queue a Job:

  ```bash
  curl -X POST http://localhost:8000/jobs/ \
      -H "Content-Type: application/json" \
      -d '{
           "queue_name": "default",
           "priority": "high",
           "payload": {"msg": "process this"}
         }'
  ```
  ```bash
  curl -X POST http://localhost:8000/jobs/ \
      -H "Content-Type: application/json" \
      -d '{
           "queue_name": "image_processing",
           "priority": "low",
           "payload": {"msg": "low priority message from queue - image_processing"}
         }'
  ```

### 2. Check Job Status:

```bash
curl http://localhost:8000/jobs/<job_id>
```

### 3. Simulate a Failing Job:
```bash

curl -X POST http://localhost:8000/jobs/ \
     -H "Content-Type: application/json" \
     -d '{
           "queue_name": "image_processing",
           "priority": "high",
           "payload": {"fail": true}
         }'
```
> This simulates a failure, and the system retries the job according to the configured strategy.

### 4. Cancel a Queued Job:
```bash
curl -X POST http://localhost:8000/jobs/<job_id>/cancel
```
> Cancels a job that is either queued or retrying.

### 5. Discover Registered Queues

```bash
curl http://localhost:8000/queues/
```
---

## Retry Mechanism
- Retries are triggered on exceptions.
- Configurable via `.env`.
- Two retry strategies are supported:
  - **fixed**: Retry after a constant delay (e.g., 1 second).
  - **exponential**: Retry after increasing delays (e.g., 1s â†’ 2s â†’ 4s â†’ 8s).
- Retry attempts are tracked via `job_retries:{job_id}` in Redis.
- Once retries are exhausted, the job moves to the DLQ.

---



## Dead-letter Queue (DLQ)

Jobs that exceed the maximum retry limit are moved to a Redis Stream called `job:dlq` for post-mortem analysis.

Each DLQ message includes:
- `job_id`
- `payload`
- `reason`

You can inspect the DLQ via Redis CLI:

```bash
# In Redis CLI (local)
127.0.0.1:6379> XRANGE job:dlq - +
```
---

## Idempotency & Deduplication

In distributed queue systems, itâ€™s common for the same job to be picked up more than once â€” either due to retries, network glitches, or multiple workers competing. Disqueue avoids this using a Redis-based locking mechanism via a reusable decorator.

The core logic is defined in `utils/deduplication.py` and applied to the job processor in `core/processor.py`:

```python
# core/worker.py

@deduplicated()
def safe_process(job_id, payload):
    logging.info(f"Processing job {job_id}")
    time.sleep(10)  # Simulated long task
    if payload.get("fail"):
        raise Exception("Simulated failure")
```

### How it works
- When a job is picked up, a Redis key `dedup:{job_id}` is set using `SET NX`, acting as a lock.
- If the key already exists, the job is considered already in progress or processed â€” so itâ€™s skipped.
- On success, mark `done` with a 24-hour TTL.
- On failure, the lock is explicitly removed to allow retries.

This ensures:
- âœ… **Safe concurrency**: In multi-worker environments, only one worker ever processes a job.
- âœ… **Retry resilience**: Failures release the lock so the job can be retried cleanly.
- âœ… **Single-worker compatibility**: Even if you have just one worker, the system behaves correctly with no risk of deadlock or side effects. It also helps in fast pre-checks before doing heavy work.



---

## Configuration

- Defined via `.env` and loaded using Pydantic in `config/settings.py`.

### Example
```env
REDIS_URL=redis://redis:6379
API_PORT=8000
RETRY_STRATEGY=exponential
```

---

## Whatâ€™s Next

### âœ… Phaseâ€¯2 â€“ Stable Core Features (Completed)
- âœ… **Job Prioritization** 
- âœ… **Job Cancellation Support**
- âœ… **Dead-letter Queue (DLQ)**
- âœ… **Exponential Backoff Retries**
- âœ… **Pluggable Retry Strategies**
- âœ… **Idempotency & Deduplication**
- âœ… **Graceful Shutdown**

Weâ€™ve completed Phaseâ€¯1 and Phase 2. Hereâ€™s a roadmap for the upcoming development phases:

### Phase 3 - Multi-Queue Architecture (In Progress)

- âœ… **Declarative Queue Registry**
- âœ… **Multi-queue + multi-priority support**
- âœ… **Modular refactor (`JobProcessor`, `StreamManager`)**
- âœ… **Enhanced API extensibility**

### Phaseâ€¯4 â€“ Advanced Features (Planned)
- **Plugin system for jobs**
- **Delayed Job Scheduling**
- **Rate Limiting**
- **Distributed Locking (Redlock)**
- **Metrics (Prometheus)**
- **CLI / Dashboard**
- **Horizontal Scaling**
- **Multi-Tenant Support**

---

## Technologies Used

- Python
- FastAPI
- Redis Streams
- Docker
- Pydantic
- Uvicorn

---

## Author

[SoeRatch](https://github.com/SoeRatch)

---

## ðŸ“„ License

This project is licensed under the [MIT License](LICENSE).

> Contributions welcome â€” feel free to open issues or PRs!
